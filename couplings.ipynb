{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" # Deactivate GPU JaX in local\n",
    "\n",
    "from utils.costs_fn_metrics import explore_cost_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import emnist, celeba_attribute\n",
    "(\n",
    "    emnist_source_data,\n",
    "    emnist_target_data,\n",
    "    emnist_one_hot_src_labels,\n",
    "    emnist_one_hot_tgt_labels,\n",
    ") = emnist(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ott.geometry.costs as costs\n",
    "from utils.ot_cost_fns import CoulombCost, HistCost\n",
    "\n",
    "metrics, comparison_metrics = explore_cost_fn(\n",
    "    X=emnist_source_data,\n",
    "    labelX=emnist_one_hot_src_labels,\n",
    "    Y=emnist_target_data,\n",
    "    labelY=emnist_one_hot_tgt_labels,\n",
    "    cost_fn=[\n",
    "        costs.PNormP(p=1),\n",
    "        costs.SqEuclidean(),\n",
    "        HistCost(),\n",
    "        costs.Euclidean(),\n",
    "        costs.Cosine(),\n",
    "        CoulombCost(),\n",
    "        costs.ElasticL1(),\n",
    "        costs.ElasticL2(),\n",
    "        costs.ElasticSTVS(),\n",
    "    ],\n",
    "    sinkhorn_matching_kwargs=dict(\n",
    "        tau_a=1.0,\n",
    "        tau_b=1.0,\n",
    "    ),\n",
    "    nbatches=100,\n",
    "    batch_size=256,\n",
    "    summarize=True,\n",
    "    save_folder=os.path.join(\"compare_cost_fn\", \"emnist_ot_batch256\"),\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "[x_train_cifar, y_train_cifar], [x_test_cifar, y_test_cifar] = (\n",
    "    tf.keras.datasets.cifar10.load_data()\n",
    ")\n",
    "\n",
    "\n",
    "def one_hot_encode(labels: np.ndarray, num_classes: int):\n",
    "    num_samples = labels.shape[0]\n",
    "    encoded_labels = np.zeros((num_samples, num_classes), dtype=int)\n",
    "    encoded_labels[np.arange(num_samples), labels.flatten()] = 1\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ott.geometry.costs as costs\n",
    "from utils.ot_cost_fns import CoulombCost, HistCost\n",
    "\n",
    "metrics, comparison_metrics = explore_cost_fn(\n",
    "    X=np.transpose(x_train_cifar, (0, 3, 1, 2)),\n",
    "    labelX=one_hot_encode(y_train_cifar, 10),\n",
    "    cost_fn=[\n",
    "        costs.SqEuclidean(),\n",
    "        HistCost(),\n",
    "        costs.PNormP(p=1),\n",
    "        costs.Euclidean(),\n",
    "        costs.Cosine(),\n",
    "        CoulombCost(),\n",
    "        costs.ElasticL1(),\n",
    "        costs.ElasticL2(),\n",
    "        costs.ElasticSTVS(),\n",
    "    ],\n",
    "    sinkhorn_matching_kwargs=dict(\n",
    "        tau_a=1.0,\n",
    "        tau_b=1.0,\n",
    "    ),\n",
    "    nbatches=50,\n",
    "    batch_size=256,\n",
    "    summarize=True,\n",
    "    save_folder=os.path.join(\"compare_cost_fn\", \"cifar_ot_batch256\"),\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celeba256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" # Deactivate GPU JaX in local\n",
    "\n",
    "from utils.datasets import celeba_attribute\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def central_crop(image: tf.Tensor, size: int) -> tf.Tensor:\n",
    "    \"\"\"Crop the center of an image to the given size.\"\"\"\n",
    "    top = (image.shape[0] - size) // 2\n",
    "    left = (image.shape[1] - size) // 2\n",
    "    return tf.image.crop_to_bounding_box(image, top, left, size, size)\n",
    "\n",
    "def process_ds(x: np.ndarray) -> tf.Tensor:\n",
    "    x = tf.cast(x, tf.float32) / 127.5 - 1.0\n",
    "    x = tf.image.resize(x, [313, 256], antialias=True)\n",
    "    x = central_crop(x, size=256)\n",
    "    x = tf.transpose(x, perm=[2, 0, 1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Callable\n",
    "import jax\n",
    "import jax.experimental.mesh_utils as mesh_utils\n",
    "import jax.sharding as sharding\n",
    "from diffusers import FlaxAutoencoderKL\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def get_vae_fns(shard: jax.sharding.Sharding) -> Tuple[Callable, Callable]:\n",
    "    fx_path = \"CompVis/stable-diffusion-v1-4\"\n",
    "    vae, vae_params = FlaxAutoencoderKL.from_pretrained(\n",
    "        fx_path, subfolder=\"vae\", revision=\"flax\", dtype=jnp.float32\n",
    "    )\n",
    "    # replicate vae params across all devices\n",
    "    vae_params = jax.device_put(vae_params, shard.replicate())\n",
    "\n",
    "    @jax.jit\n",
    "    def encode_fn(image_batch: jax.Array) -> jax.Array:\n",
    "        latent_out = vae.apply({\"params\": vae_params}, image_batch, method=vae.encode)\n",
    "        latent = latent_out.latent_dist.mode()\n",
    "        latent = (latent * vae.config.scaling_factor).transpose(0, 3, 1, 2)\n",
    "        return jax.lax.with_sharding_constraint(latent, shard)\n",
    "\n",
    "    @jax.jit\n",
    "    def decode_fn(latent_batch: jax.Array) -> jax.Array:\n",
    "        image_out = vae.apply(\n",
    "            {\"params\": vae_params},\n",
    "            latent_batch / vae.config.scaling_factor,\n",
    "            method=vae.decode,\n",
    "        )\n",
    "        return jax.lax.with_sharding_constraint(image_out.sample, shard)\n",
    "\n",
    "    return encode_fn, decode_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [11:58<00:00, 69.57it/s]  \n",
      "100%|██████████| 10521/10521 [03:30<00:00, 50.10it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load normal dataset \n",
    "N = 50_000\n",
    "celebaX, celebaY, _, _ = celeba_attribute(\n",
    "    split='train',\n",
    "    attribute_id=15,\n",
    "    map_forward=True,\n",
    "    batch_size=256,\n",
    "    overfit_to_one_batch = False,\n",
    "    nsamples = N,\n",
    ")\n",
    "\n",
    "plt.imshow(celebaX[0])\n",
    "plt.show()\n",
    "plt.imshow(np.transpose(process_ds(celebaX[0]), (1,2,0)))\n",
    "\n",
    "# We need to transpose to get it into correct format for plotting (as explore internally transposes to get them all into the format)\n",
    "celebaX = celebaX.transpose(0, 3, 1, 2)\n",
    "celebaY = celebaY.transpose(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_devices = len(jax.devices())\n",
    "# shard needs to have same number of dimensions as the input\n",
    "devices = mesh_utils.create_device_mesh((num_devices, 1, 1, 1))\n",
    "shard = sharding.PositionalSharding(devices)\n",
    "vae_encode_fn, vae_decode_fn = get_vae_fns(shard)\n",
    "\n",
    "# Load embedded dataset\n",
    "celeba_embX, celeba_embY, celeba_labelX, celeba_labelY = celeba_attribute(\n",
    "    split='train',\n",
    "    attribute_id=15,\n",
    "    map_forward=True,\n",
    "    batch_size=256,\n",
    "    overfit_to_one_batch = False,\n",
    "    nsamples = N,\n",
    "    vae_encode_fn = vae_encode_fn,\n",
    "    preprocess_fn = process_ds,\n",
    ")\n",
    "\n",
    "celeba_labelX[celeba_labelX == -1] = 0\n",
    "celeba_labelY[celeba_labelY == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(celeba_embX[0,:,:,:], (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.costs_fn_metrics import explore_cost_fn\n",
    "import ott.geometry.costs as costs\n",
    "from utils.ot_cost_fns import CoulombCost, HistCost\n",
    "\n",
    "metrics, comparison_metrics = explore_cost_fn(\n",
    "    X=celeba_embX,\n",
    "    labelX=celeba_labelX,\n",
    "    Y=celeba_embY,\n",
    "    labelY=celeba_labelY,\n",
    "    cost_fn=[\n",
    "        costs.SqEuclidean(),\n",
    "        HistCost(),\n",
    "        costs.PNormP(p=1),\n",
    "        costs.Euclidean(),\n",
    "        costs.Cosine(),\n",
    "        CoulombCost(),\n",
    "        costs.ElasticL1(),\n",
    "        costs.ElasticL2(),\n",
    "        costs.ElasticSTVS(),\n",
    "    ],\n",
    "    sinkhorn_matching_kwargs=dict(\n",
    "        tau_a=1.0,\n",
    "        tau_b=1.0,\n",
    "    ),\n",
    "    nbatches=50,\n",
    "    batch_size=256,\n",
    "    summarize=True,\n",
    "    save_folder=os.path.join(\"compare_cost_fn\", \"celeba_ot_batch256\"),\n",
    "    overwrite=True,\n",
    "    decodedX=celebaX, \n",
    "    decodedY=celebaY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uot-fm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
